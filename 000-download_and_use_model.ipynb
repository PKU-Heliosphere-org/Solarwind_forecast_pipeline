{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4062562-c1d2-4136-af72-0a5709f0f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\87488\\.conda\\envs\\py39env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# First version of the forecast pipeline program\n",
    "# Author: Rong Lin\n",
    "# Date: 2024/08/08\n",
    "\n",
    "# please use py39env on RONG's windows laptop.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pfsspy\n",
    "\n",
    "# C:\\Users\\87488\\AppData\\Local\\Temp\\ipykernel_3384\\1689277766.py:6: DeprecationWarning: \n",
    "# Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
    "# (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
    "# but was not found to be installed on your system.\n",
    "# If this would cause problems for you,\n",
    "# please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
    "\n",
    "# solution: pip install pyarrow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc7408a-d8b5-489f-b6ce-eaccce8ea2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfsspy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b66691e-52f4-4e09-9fba-534798e46359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sunpy\n",
    "sunpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80392248-b3f0-4aac-97d0-c5fa1f35bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date          3_days_ago          4_days_ago  \\\n",
      "0 2021-06-11 14:00:00 2021-06-08 14:00:00 2021-06-07 14:00:00   \n",
      "1 2021-06-11 15:00:00 2021-06-08 15:00:00 2021-06-07 15:00:00   \n",
      "2 2021-06-12 15:00:00 2021-06-09 15:00:00 2021-06-08 15:00:00   \n",
      "3 2021-06-12 16:00:00 2021-06-09 16:00:00 2021-06-08 16:00:00   \n",
      "4 2021-06-12 17:00:00 2021-06-09 17:00:00 2021-06-08 17:00:00   \n",
      "5 2021-06-13 16:00:00 2021-06-10 16:00:00 2021-06-09 16:00:00   \n",
      "\n",
      "           5_days_ago          6_days_ago  \n",
      "0 2021-06-06 14:00:00 2021-06-05 14:00:00  \n",
      "1 2021-06-06 15:00:00 2021-06-05 15:00:00  \n",
      "2 2021-06-07 15:00:00 2021-06-06 15:00:00  \n",
      "3 2021-06-07 16:00:00 2021-06-06 16:00:00  \n",
      "4 2021-06-07 17:00:00 2021-06-06 17:00:00  \n",
      "5 2021-06-08 16:00:00 2021-06-07 16:00:00  \n"
     ]
    }
   ],
   "source": [
    "# input: epoches to be forecast\n",
    "# TODO: make the slide buttton...\n",
    "date_dict = {'date':[datetime.datetime(2021, 6, 11, 14),\n",
    "                     datetime.datetime(2021, 6, 11, 15),\n",
    "                     datetime.datetime(2021, 6, 12, 15),\n",
    "                     datetime.datetime(2021, 6, 12, 16),\n",
    "                     datetime.datetime(2021, 6, 12, 17),\n",
    "                     datetime.datetime(2021, 6, 13, 16)]}\n",
    "\n",
    "# calculate the date for 3, 4, 5 and 6 days ago, for each epoch\n",
    "date_df = pd.DataFrame(date_dict)\n",
    "date_df['3_days_ago'] = date_df['date'] + datetime.timedelta(days=-3)\n",
    "date_df['4_days_ago'] = date_df['date'] + datetime.timedelta(days=-4)\n",
    "date_df['5_days_ago'] = date_df['date'] + datetime.timedelta(days=-5)\n",
    "date_df['6_days_ago'] = date_df['date'] + datetime.timedelta(days=-6)\n",
    "print(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea40261-d647-4b12-963b-231ac4e1c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             uniq_date path\n",
      "0  2021-06-05 14:00:00     \n",
      "1  2021-06-05 15:00:00     \n",
      "2  2021-06-06 14:00:00     \n",
      "3  2021-06-06 15:00:00     \n",
      "4  2021-06-06 16:00:00     \n",
      "5  2021-06-06 17:00:00     \n",
      "6  2021-06-07 14:00:00     \n",
      "7  2021-06-07 15:00:00     \n",
      "8  2021-06-07 16:00:00     \n",
      "9  2021-06-07 17:00:00     \n",
      "10 2021-06-08 14:00:00     \n",
      "11 2021-06-08 15:00:00     \n",
      "12 2021-06-08 16:00:00     \n",
      "13 2021-06-08 17:00:00     \n",
      "14 2021-06-09 15:00:00     \n",
      "15 2021-06-09 16:00:00     \n",
      "16 2021-06-09 17:00:00     \n",
      "17 2021-06-10 16:00:00     \n"
     ]
    }
   ],
   "source": [
    "# merge all dates from 3, 4, 5, and 6 days ago,\n",
    "# so that we don't need to download for multiple times\n",
    "melted_df = pd.melt(date_df, value_vars=['3_days_ago', '4_days_ago', '5_days_ago', '6_days_ago'], value_name='uniq_date')\n",
    "unique_dates = pd.to_datetime(melted_df['uniq_date']).drop_duplicates()\n",
    "unique_sorted_dates = unique_dates.sort_values().reset_index(drop=True)\n",
    "unique_sorted_dates = pd.DataFrame(unique_sorted_dates)\n",
    "unique_sorted_dates['path'] =''\n",
    "print(unique_sorted_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704f72d1-3e45-49f7-ba10-ae90e1ce6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOADING dependencies: modules, constants and functions\n",
    "import os, gzip\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "    \n",
    "URL_HEADER = 'https://gong2.nso.edu/oQR/zqs/'\n",
    "FOLDER = 'gong_data/'\n",
    "\n",
    "def find_nearest_file(time_filename_dict, input_datetime, time_diff=0.5):\n",
    "    # time diff is in hour(s)\n",
    "    # the dict: key is time\n",
    "    # find corresponding value in dict, with the key-time nearest to the input_datetime\n",
    "    if not time_filename_dict:\n",
    "        return None\n",
    "    nearest_dt = min(time_filename_dict.keys(), key=lambda dt: abs(dt - input_datetime))\n",
    "    if abs(nearest_dt - input_datetime) > datetime.timedelta(hours=time_diff):\n",
    "        return None\n",
    "    else:\n",
    "        return time_filename_dict[nearest_dt]\n",
    "        \n",
    "def get_url_of_downloading_page(date_inp):\n",
    "    year2_month_day_str = str(date_inp.year)[-2:] + '%02d' % date_inp.month + '%02d' % date_inp.day\n",
    "    url = URL_HEADER + year4_month_str + '/mrzqs' + year2_month_day_str + '/'\n",
    "    return url\n",
    "    \n",
    "def get_downloading_page_dict(url,date_inp):\n",
    "    # analyse the downlaoding page, and get the filename list with corresponding file url\n",
    "    r0 = requests.get(url)\n",
    "    soup0 = BeautifulSoup(r0.content)\n",
    "    links0 = soup0.findAll('a')\n",
    "    # build dict for that page\n",
    "    res = dict()\n",
    "    for link0 in links0:\n",
    "        if link0['href'].startswith('mrzqs'):\n",
    "            url1 = url + link0['href']\n",
    "            splitted_href = link0['href'].split('t')\n",
    "            hours_str = splitted_href[1][:2]\n",
    "            mins_str = splitted_href[1][2:4]\n",
    "            file_time = datetime.datetime(date_inp.year,date_inp.month,date_inp.day,int(hours_str),int(mins_str))\n",
    "            res[file_time] = url1\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6d2a57-4894-41ee-8127-cc0236ff0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOADING required files\n",
    "year_month_day_previous_i = None\n",
    "for i_date in range(len(unique_sorted_dates)):\n",
    "    date_input = unique_sorted_dates['uniq_date'][i_date]\n",
    "    date_input = date_input.to_pydatetime()\n",
    "    year4_month_str = str(date_input.year) + '%02d' % date_input.month\n",
    "    year2_month_day_str = str(date_input.year)[-2:]+'%02d'%date_input.month+'%02d'%date_input.day\n",
    "    if (year_month_day_previous_i is not None) and (year2_month_day_str == year_month_day_previous_i):\n",
    "        pass\n",
    "        # still use the \"time_filename_dict_this_day\"\n",
    "    else:\n",
    "        # get url of downloading page\n",
    "        url0 = get_url_of_downloading_page(date_input)\n",
    "        \n",
    "        time_filename_dict_this_day = get_downloading_page_dict(url0,date_input)   \n",
    "        \n",
    "        # analyse the downlaoding page, and get the filename list with corresponding file url\n",
    "        r0 = requests.get(url0)\n",
    "        soup0 = BeautifulSoup(r0.content)\n",
    "        links0 = soup0.findAll('a')\n",
    "\n",
    "        # build dict for that page\n",
    "        time_filename_dict_this_day = dict()\n",
    "        for link0 in links0:\n",
    "            if link0['href'].startswith('mrzqs'):\n",
    "                url1 = url0 + link0['href']\n",
    "                splitted_href = link0['href'].split('t')\n",
    "                hours_str = splitted_href[1][:2]\n",
    "                mins_str = splitted_href[1][2:4]\n",
    "                file_time = datetime.datetime(date_input.year,date_input.month,date_input.day,int(hours_str),int(mins_str))\n",
    "                time_filename_dict_this_day[file_time] = url1\n",
    "        year_month_day_previous_i = year2_month_day_str        \n",
    "    \n",
    "    gz_fileurl = find_nearest_file(time_filename_dict_this_day, date_input)\n",
    "    if gz_fileurl is None:\n",
    "        gz_filename = None\n",
    "        path_file_full = None\n",
    "    else:\n",
    "        gz_filename = gz_fileurl.split('/')[-1] # NoneType does not have it\n",
    "        path = FOLDER + year4_month_str + '/' + gz_filename[:11]+'/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        path_file = path + gz_filename\n",
    "        if not os.path.exists(path_file):\n",
    "            r2 = requests.get(gz_fileurl)\n",
    "            data = r2.content\n",
    "            with open(path_file, 'wb') as f:\n",
    "                f.write(data)\n",
    "            path_file_full = path_file.replace('.gz', '')\n",
    "            file_data = gzip.GzipFile(path_file)\n",
    "            with open(path_file_full, \"wb+\") as pfu:\n",
    "                pfu.write(file_data.read())\n",
    "            file_data.close()\n",
    "        else:\n",
    "            path_file_full = path_file.replace('.gz', '')\n",
    "            pass\n",
    "    # print(date_input, year2_month_day_str, url0, gz_filename, path_file_full)\n",
    "    unique_sorted_dates.loc[i_date,'path'] = path_file_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3605c668-1032-434f-968e-2c6d5dd669ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  date          3_days_ago          4_days_ago  \\\n",
      "0 2021-06-11 14:00:00 2021-06-08 14:00:00 2021-06-07 14:00:00   \n",
      "1 2021-06-11 15:00:00 2021-06-08 15:00:00 2021-06-07 15:00:00   \n",
      "2 2021-06-12 15:00:00 2021-06-09 15:00:00 2021-06-08 15:00:00   \n",
      "3 2021-06-12 16:00:00 2021-06-09 16:00:00 2021-06-08 16:00:00   \n",
      "4 2021-06-12 17:00:00 2021-06-09 17:00:00 2021-06-08 17:00:00   \n",
      "5 2021-06-13 16:00:00 2021-06-10 16:00:00 2021-06-09 16:00:00   \n",
      "\n",
      "           5_days_ago          6_days_ago  \\\n",
      "0 2021-06-06 14:00:00 2021-06-05 14:00:00   \n",
      "1 2021-06-06 15:00:00 2021-06-05 15:00:00   \n",
      "2 2021-06-07 15:00:00 2021-06-06 15:00:00   \n",
      "3 2021-06-07 16:00:00 2021-06-06 16:00:00   \n",
      "4 2021-06-07 17:00:00 2021-06-06 17:00:00   \n",
      "5 2021-06-08 16:00:00 2021-06-07 16:00:00   \n",
      "\n",
      "                                     path_3_days_ago  \\\n",
      "0                                               None   \n",
      "1  gong_data/202106/mrzqs210608/mrzqs210608t1514c...   \n",
      "2  gong_data/202106/mrzqs210609/mrzqs210609t1434c...   \n",
      "3  gong_data/202106/mrzqs210609/mrzqs210609t1614c...   \n",
      "4  gong_data/202106/mrzqs210609/mrzqs210609t1714c...   \n",
      "5  gong_data/202106/mrzqs210610/mrzqs210610t1614c...   \n",
      "\n",
      "                                     path_4_days_ago  \\\n",
      "0  gong_data/202106/mrzqs210607/mrzqs210607t1404c...   \n",
      "1  gong_data/202106/mrzqs210607/mrzqs210607t1504c...   \n",
      "2  gong_data/202106/mrzqs210608/mrzqs210608t1514c...   \n",
      "3  gong_data/202106/mrzqs210608/mrzqs210608t1614c...   \n",
      "4  gong_data/202106/mrzqs210608/mrzqs210608t1704c...   \n",
      "5  gong_data/202106/mrzqs210609/mrzqs210609t1614c...   \n",
      "\n",
      "                                     path_5_days_ago  \\\n",
      "0  gong_data/202106/mrzqs210606/mrzqs210606t1404c...   \n",
      "1  gong_data/202106/mrzqs210606/mrzqs210606t1514c...   \n",
      "2  gong_data/202106/mrzqs210607/mrzqs210607t1504c...   \n",
      "3  gong_data/202106/mrzqs210607/mrzqs210607t1554c...   \n",
      "4  gong_data/202106/mrzqs210607/mrzqs210607t1714c...   \n",
      "5  gong_data/202106/mrzqs210608/mrzqs210608t1614c...   \n",
      "\n",
      "                                     path_6_days_ago  all_paths_present  \n",
      "0  gong_data/202106/mrzqs210605/mrzqs210605t1414c...              False  \n",
      "1  gong_data/202106/mrzqs210605/mrzqs210605t1514c...               True  \n",
      "2  gong_data/202106/mrzqs210606/mrzqs210606t1514c...               True  \n",
      "3  gong_data/202106/mrzqs210606/mrzqs210606t1604c...               True  \n",
      "4  gong_data/202106/mrzqs210606/mrzqs210606t1714c...               True  \n",
      "5  gong_data/202106/mrzqs210607/mrzqs210607t1554c...               True  >\n"
     ]
    }
   ],
   "source": [
    "# arrange dataframe\n",
    "date_df = date_df.merge(unique_sorted_dates, left_on='3_days_ago', right_on='uniq_date',\n",
    "                        how='left').drop(columns=['uniq_date']).rename(columns={'path': 'path_3_days_ago'})\n",
    "date_df = date_df.merge(unique_sorted_dates, left_on='4_days_ago', right_on='uniq_date',\n",
    "                        how='left').drop(columns=['uniq_date']).rename(columns={'path': 'path_4_days_ago'})\n",
    "date_df = date_df.merge(unique_sorted_dates, left_on='5_days_ago', right_on='uniq_date',\n",
    "                        how='left').drop(columns=['uniq_date']).rename(columns={'path': 'path_5_days_ago'})\n",
    "date_df = date_df.merge(unique_sorted_dates, left_on='6_days_ago', right_on='uniq_date',\n",
    "                        how='left').drop(columns=['uniq_date']).rename(columns={'path': 'path_6_days_ago'})\n",
    "\n",
    "conditions = date_df[['path_3_days_ago', 'path_4_days_ago', 'path_5_days_ago', 'path_6_days_ago']].notnull().all(axis=1)\n",
    "date_df['all_paths_present'] = conditions\n",
    "print(date_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c6856f-977b-4527-91d6-0532cee523e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# GONG, PFSS and torch dependencies: modules, constants and functions\n",
    "import sunpy.map\n",
    "import pfsspy\n",
    "from sunpy.sun import constants\n",
    "import astropy.units as u\n",
    "\n",
    "import torch\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nrho = 30\n",
    "rss = 2.5\n",
    "\n",
    "def get_gong_map_obj(path_to_file):\n",
    "    # create gong map object\n",
    "    gmap_temp = sunpy.map.Map(path_to_file)\n",
    "    gmap = sunpy.map.Map(gmap_temp.data - np.mean(gmap_temp.data), gmap_temp.meta)\n",
    "\n",
    "    # added by RONG 2024/8/15, to prevent some \n",
    "    gmap.meta['rsun_ref'] = constants.radius / u.m\n",
    "    gmap.meta['bunit'] = 'G'\n",
    "    return gmap\n",
    "\n",
    "def get_ss_graphs(df, i_df):\n",
    "    col = df.iloc[i_df]\n",
    "    \n",
    "    gong_map_3 = get_gong_map_obj(col['path_3_days_ago'])\n",
    "    input_3 = pfsspy.Input(gong_map_3, nrho, rss)\n",
    "    output_3 = pfsspy.pfss(input_3)\n",
    "    graph_3 = output_3.source_surface_br.data\n",
    "    lat_3 = int(gong_map_3.carrington_latitude.to_value().round())\n",
    "    sub_graph_3 = graph_3[lat_3+90-45:lat_3+90+45,90:-90]\n",
    "    \n",
    "    gong_map_4 = get_gong_map_obj(col['path_4_days_ago'])\n",
    "    input_4 = pfsspy.Input(gong_map_4, nrho, rss)\n",
    "    output_4 = pfsspy.pfss(input_4)\n",
    "    graph_4 = output_4.source_surface_br.data\n",
    "    lat_4 = int(gong_map_4.carrington_latitude.to_value().round())\n",
    "    sub_graph_4 = graph_4[lat_4+90-45:lat_4+90+45,90:-90]\n",
    "    \n",
    "    gong_map_5 = get_gong_map_obj(col['path_5_days_ago'])\n",
    "    input_5 = pfsspy.Input(gong_map_5, nrho, rss)\n",
    "    output_5 = pfsspy.pfss(input_5)\n",
    "    graph_5 = output_5.source_surface_br.data\n",
    "    lat_5 = int(gong_map_5.carrington_latitude.to_value().round())\n",
    "    sub_graph_5 = graph_5[lat_5+90-45:lat_5+90+45,90:-90]\n",
    "    \n",
    "    gong_map_6 = get_gong_map_obj(col['path_6_days_ago'])\n",
    "    input_6 = pfsspy.Input(gong_map_6, nrho, rss)\n",
    "    output_6 = pfsspy.pfss(input_6)\n",
    "    graph_6 = output_6.source_surface_br.data\n",
    "    lat_6 = int(gong_map_6.carrington_latitude.to_value().round())\n",
    "    sub_graph_6 = graph_6[lat_6+90-45:lat_6+90+45,90:-90]\n",
    "\n",
    "    ss_graphs = np.array([sub_graph_6,sub_graph_5,sub_graph_4,sub_graph_3])\n",
    "    \n",
    "    return ss_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26a9da6-d63f-4feb-80f5-492815e232e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.5 s\n",
      "Wall time: 36.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 90, 180])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "can_forecast_indices = date_df.index[date_df['all_paths_present'] == True].tolist()\n",
    "can_forecast_indices\n",
    "\n",
    "input_for_model = list()\n",
    "for i in can_forecast_indices:\n",
    "    input_for_model.append(get_ss_graphs(date_df,i))\n",
    "    \n",
    "input_for_model = np.array(input_for_model)\n",
    "input_for_model = torch.from_numpy(input_for_model)\n",
    "input_for_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73367ad2-0677-4a2f-a711-9f4ea53bf5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=16, kernel_size=(10,10), stride=1, padding=5\n",
    "        )\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=(3, 3), stride=1, padding=1\n",
    "        )\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=(3, 3), stride=1, padding=1\n",
    "        )\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(64 * 50, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x * 1200.0\n",
    "        output = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0faece5a-3500-4ccd-ad56-3e8ffa39bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_1 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss1.pt\")\n",
    "# MODEL_2 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss2.pt\")\n",
    "# MODEL_3 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss3.pt\")\n",
    "# MODEL_4 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss4.pt\")\n",
    "# MODEL_5 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss5.pt\")\n",
    "# MODEL_6 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss6.pt\")\n",
    "# MODEL_7 = torch.load(\"./Models/\" + \"20230613MODEL_temporary_min_validation_loss7.pt\")\n",
    "\n",
    "# torch.save(MODEL_1.state_dict(), \"./Models/published_model_1.pt\")\n",
    "# torch.save(MODEL_2.state_dict(), \"./Models/published_model_2.pt\")\n",
    "# torch.save(MODEL_3.state_dict(), \"./Models/published_model_3.pt\")\n",
    "# torch.save(MODEL_4.state_dict(), \"./Models/published_model_4.pt\")\n",
    "# torch.save(MODEL_5.state_dict(), \"./Models/published_model_5.pt\")\n",
    "# torch.save(MODEL_6.state_dict(), \"./Models/published_model_6.pt\")\n",
    "# torch.save(MODEL_7.state_dict(), \"./Models/published_model_7.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50b25d3-2dd1-4aa9-9e19-a096912a89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1 = Net().to(DEVICE)\n",
    "MODEL_1.load_state_dict(torch.load(\"./Models/published_model_1.pt\",weights_only=True))\n",
    "MODEL_2 = Net().to(DEVICE)\n",
    "MODEL_2.load_state_dict(torch.load(\"./Models/published_model_2.pt\",weights_only=True))\n",
    "MODEL_3 = Net().to(DEVICE)\n",
    "MODEL_3.load_state_dict(torch.load(\"./Models/published_model_3.pt\",weights_only=True))\n",
    "MODEL_4 = Net().to(DEVICE)\n",
    "MODEL_4.load_state_dict(torch.load(\"./Models/published_model_4.pt\",weights_only=True))\n",
    "MODEL_5 = Net().to(DEVICE)\n",
    "MODEL_5.load_state_dict(torch.load(\"./Models/published_model_5.pt\",weights_only=True))\n",
    "MODEL_6 = Net().to(DEVICE)\n",
    "MODEL_6.load_state_dict(torch.load(\"./Models/published_model_6.pt\",weights_only=True))\n",
    "MODEL_7 = Net().to(DEVICE)\n",
    "MODEL_7.load_state_dict(torch.load(\"./Models/published_model_7.pt\",weights_only=True))\n",
    "\n",
    "for MODEL in [MODEL_1, MODEL_2, MODEL_3, MODEL_4, MODEL_5, MODEL_6, MODEL_7]:\n",
    "    MODEL.eval();\n",
    "    \n",
    "x = input_for_model\n",
    "x = x.type(torch.FloatTensor)\n",
    "x = x.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b75082-7c20-4edd-9eb8-27d7ed48bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['date'] + [f'forecast_{i}' for i in range(8)]\n",
    "new_df = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "new_df['date'] = date_df['date']\n",
    "for col in new_df.columns[1:]:\n",
    "    new_df[col] = np.nan\n",
    "\n",
    "result_df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52328b80-f87e-41b6-a226-8265a93f0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,8):\n",
    "    model = eval(f\"MODEL_{i}\")\n",
    "\n",
    "    forecast_values = model(x)\n",
    "    forecast_values = forecast_values.cpu().detach().numpy().flatten()\n",
    "\n",
    "    result_df.loc[can_forecast_indices, f'forecast_{i}'] = forecast_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc5398c2-0825-49c2-89d2-8f7c795895bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>forecast_0</th>\n",
       "      <th>forecast_1</th>\n",
       "      <th>forecast_2</th>\n",
       "      <th>forecast_3</th>\n",
       "      <th>forecast_4</th>\n",
       "      <th>forecast_5</th>\n",
       "      <th>forecast_6</th>\n",
       "      <th>forecast_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-11 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-11 15:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>387.453003</td>\n",
       "      <td>387.208496</td>\n",
       "      <td>432.113434</td>\n",
       "      <td>402.048035</td>\n",
       "      <td>410.836792</td>\n",
       "      <td>410.227264</td>\n",
       "      <td>407.636230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-12 15:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.751831</td>\n",
       "      <td>347.824615</td>\n",
       "      <td>388.722961</td>\n",
       "      <td>367.060760</td>\n",
       "      <td>387.073364</td>\n",
       "      <td>365.191986</td>\n",
       "      <td>345.038940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-12 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335.369141</td>\n",
       "      <td>343.673065</td>\n",
       "      <td>392.021545</td>\n",
       "      <td>373.312653</td>\n",
       "      <td>386.911774</td>\n",
       "      <td>369.612701</td>\n",
       "      <td>348.237976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-12 17:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335.952057</td>\n",
       "      <td>342.251831</td>\n",
       "      <td>393.836548</td>\n",
       "      <td>376.704559</td>\n",
       "      <td>387.444733</td>\n",
       "      <td>372.574219</td>\n",
       "      <td>347.072601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-06-13 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>338.898285</td>\n",
       "      <td>348.385803</td>\n",
       "      <td>370.291687</td>\n",
       "      <td>381.788483</td>\n",
       "      <td>370.776215</td>\n",
       "      <td>369.862488</td>\n",
       "      <td>336.146027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  forecast_0  forecast_1  forecast_2  forecast_3  \\\n",
       "0 2021-06-11 14:00:00         NaN         NaN         NaN         NaN   \n",
       "1 2021-06-11 15:00:00         NaN  387.453003  387.208496  432.113434   \n",
       "2 2021-06-12 15:00:00         NaN  333.751831  347.824615  388.722961   \n",
       "3 2021-06-12 16:00:00         NaN  335.369141  343.673065  392.021545   \n",
       "4 2021-06-12 17:00:00         NaN  335.952057  342.251831  393.836548   \n",
       "5 2021-06-13 16:00:00         NaN  338.898285  348.385803  370.291687   \n",
       "\n",
       "   forecast_4  forecast_5  forecast_6  forecast_7  \n",
       "0         NaN         NaN         NaN         NaN  \n",
       "1  402.048035  410.836792  410.227264  407.636230  \n",
       "2  367.060760  387.073364  365.191986  345.038940  \n",
       "3  373.312653  386.911774  369.612701  348.237976  \n",
       "4  376.704559  387.444733  372.574219  347.072601  \n",
       "5  381.788483  370.776215  369.862488  336.146027  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dba3c8-d373-44b3-9d86-8a3c871e91b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "py39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
